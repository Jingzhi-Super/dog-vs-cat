{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization, GlobalMaxPooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, Activation, PReLU, LeakyReLU\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import time\n",
    "from keras.callbacks import History \n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "\n",
    "# deal with potential warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read processed array data, usually takes 1 or 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.load('data/data1.npy')\n",
    "data2 = np.load('data/data2.npy')\n",
    "data3 = np.load('data/data3.npy')\n",
    "data4 = np.load('data/data4.npy')\n",
    "data = np.concatenate((data1,data2,data3,data4))\n",
    "x_test = np.load('data/testdata.npy')\n",
    "testID = np.load('data/testdataID.npy')\n",
    "\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train/test split and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test ratio\n",
    "ratio = 0.8\n",
    "\n",
    "# shuffle dataset\n",
    "seed(1)\n",
    "np.random.shuffle(data)\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "for index,item in enumerate(data):\n",
    "    if index<len(data)*ratio:\n",
    "        x_train.append(item[0])\n",
    "        y_train.append(item[1])\n",
    "    else:\n",
    "        x_dev.append(item[0])\n",
    "        y_dev.append(item[1])\n",
    "    \n",
    "x_train = np.asarray(x_train)\n",
    "x_dev = np.asarray(x_dev)\n",
    "y_train = np.asarray(y_train)\n",
    "y_dev = np.asarray(y_dev)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "# normalize input\n",
    "x_train /= 255\n",
    "x_dev /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 2)[:,-1]\n",
    "y_dev = keras.utils.to_categorical(y_dev, 2)[:,-1]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print('x_dev shape:', x_dev.shape)\n",
    "print(x_dev.shape[0], 'validtion samples')\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/save_data', (x_train, y_train, x_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train, x_dev, y_dev) = np.load('data/save_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=0.001, dropout_rate=[0.5], alpha=0.0001):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), strides = (1, 1), input_shape=(64, 64, 3), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(16, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(32, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(64, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(64, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(128, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Conv2D(128, (3, 3), strides = (1, 1), border_mode='same', W_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "#     model.add(GlobalMaxPooling2D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout_rate[0]))\n",
    "    model.add(Dense(512))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(dropout_rate[1]))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=lr, amsgrad=True),\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks, early-stopping\n",
    "callbacks = keras.callbacks.ModelCheckpoint('prelu_model_1.h5', \n",
    "                                             monitor='val_acc', verbose=1, save_best_only=True, \n",
    "                                             save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "callback_list = [callbacks]\n",
    "\n",
    "# hyper-parameters\n",
    "param = {'lr':[0.001],\n",
    "#          'batch_size':np.power(2,np.arange(5,7)),\n",
    "         'dropout_rate':np.round(np.arange(0.4,0.9,0.1),1)}\n",
    "\n",
    "param_list = list(ParameterSampler(param, n_iter=5))\n",
    "rounded_list = [dict((k, v) for (k, v) in d.items()) for d in param_list]\n",
    "print(rounded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn wrapper training\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1, \n",
    "                        epochs=100,\n",
    "                        lr=0.001,\n",
    "                        batch_size=64,\n",
    "                        dropout_rate=[0.7],\n",
    "                        alpha=0.01,\n",
    "                        validation_data=(x_dev,y_dev),\n",
    "                        callbacks=callback_list)\n",
    "train_history = model.fit(x_train, y_train)\n",
    "#     model.model.save('my_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training\n",
    "temp_weights = [layer.get_weights() for layer in new_model.layers]\n",
    "model = create_model(lr=0.0001,\n",
    "                     dropout_rate=[0.7],\n",
    "                     alpha=0.01)\n",
    "for j in range(len(temp_weights)):\n",
    "    model.layers[j].set_weights(temp_weights[j])\n",
    "    \n",
    "train_history = model.fit(x_train, y_train, verbose=1,\n",
    "                          batch_size=64,\n",
    "                          validation_data=(x_dev,y_dev), \n",
    "                          epochs=100,\n",
    "                          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weights = [layer.get_weights() for layer in new_model.layers]\n",
    "\n",
    "model = create_model(lr=1e-6,\n",
    "                     dropout_rate=[0.7, 0.7],\n",
    "                     alpha=0.01)\n",
    "\n",
    "for j in range(len(temp_weights)):\n",
    "    model.layers[j].set_weights(temp_weights[j])\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2,\n",
    "                             rotation_range=20,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), \n",
    "                    verbose=1,\n",
    "                    steps_per_epoch=len(x_train) // 32,\n",
    "                    validation_data=(x_dev,y_dev), \n",
    "                    epochs=100,\n",
    "                    callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('prelu_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "y_pre = np.round(new_model.predict(x_dev, verbose=1))\n",
    "confusion_matrix(y_dev, y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(train_history.history['acc'])\n",
    "plt.plot(train_history.history['val_acc'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offline working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('prelu_model.h5')\n",
    "new_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict on test set \n",
    "score = np.round(new_model.predict(x_test, verbose=1)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = pd.Series(testID)\n",
    "sub['label'] = score\n",
    "sub.to_csv('DvCsubmission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some other fur balls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'pics/wechat.jpg'\n",
    "img = image.load_img(img_path, target_size=(64, 64))\n",
    "x = image.img_to_array(img) / 255\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "x = np.expand_dims(x, axis=0)\n",
    "# print(model.predict(x))\n",
    "print(['cat', 'dog'][int(new_model.predict(x))])\n",
    "print(new_model.predict(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
